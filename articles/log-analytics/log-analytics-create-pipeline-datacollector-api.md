---
title: Создание конвейера данных с помощью API сборщика данных службы Azure Log Analytics | Документация Майкрософт
description: API сборщика данных HTTP в Log Analytics можно использовать для добавления данных POST JSON в репозиторий Log Analytics из любого клиента, который может вызывать REST API. В статье содержится описание автоматической отправки данных, которые хранятся в файлах.
services: log-analytics
documentationcenter: ''
author: mgoedtel
manager: carmonm
editor: ''
ms.assetid: ''
ms.service: log-analytics
ms.workload: na
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: conceptual
ms.date: 08/09/2018
ms.author: magoedte
ms.component: ''
ms.openlocfilehash: 180f1a39b92dd699fa114cb98a5842b0ab0dc89a
ms.sourcegitcommit: 63613e4c7edf1b1875a2974a29ab2a8ce5d90e3b
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 08/29/2018
ms.locfileid: "43190544"
---
# <a name="create-a-data-pipeline-with-the-data-collector-api"></a>Создание конвейера данных с помощью API сборщика данных

[API сборщика данных службы Azure Log Analytics](log-analytics-data-collector-api.md) позволяет импортировать любые пользовательские данные в Log Analytics. Единственным требованием к данным является то, что они должны находиться в формате JSON и быть разделены пакетами размером не более 30 МБ. Это полностью гибкий механизм, который можно подключить многими способами: от передачи данных непосредственно из приложения до одноразовых отправок adhoc. В этой статье будут описаны некоторые отправные точки для общего сценария, например, необходимость отправлять данные, хранящиеся в файлах, на регулярной, автоматизированной основе. Хотя представленный здесь конвейер не будет наиболее эффективным или оптимизированным, он призван служить отправной точкой для создания собственного рабочего конвейера.

## <a name="example-problem"></a>Пример проблемы
В оставшейся части этой статьи будут рассмотрены данные просмотра страниц в Application Insights. В текущем гипотетическом сценарии необходимо сопоставить географическую информацию, собранную по умолчанию пакетами SDK Application Insights, с пользовательскими данными, которые содержат информацию о населении каждой страны в мире. Это требуется для того, чтобы определить, куда необходимо потратить большую часть маркетингового бюджета. 

Для этой цели используются такие публичные источники данных, как [World Population Prospects 2017](https://esa.un.org/unpd/wpp/) (Мировые демографические перспективы 2017). Данные будут выглядеть следующим образом.

![Пример простой схемы](./media/log-analytics-create-pipeline-datacollector-api/example-simple-schema-01.png)

В данном примере предполагается, что будет выполнена отправка нового файла, который содержит информацию за последний год, с данными, как только они станут доступными.

## <a name="general-design"></a>Общий макет
Чтобы сконструировать конвейер, используется стандартный тип данных формата ETL. Архитектура будет выглядеть следующим образом.

![Архитектура конвейера сбора данных](./media/log-analytics-create-pipeline-datacollector-api/data-pipeline-dataflow-architecture.png)

Дополнительные сведения о создании данных см. в статье [Передача данных изображений в облако с помощью службы хранилища Azure](../storage/blobs/storage-upload-process-images.md). Здесь описан процесс выбора потока, после того как новый файл был отправлен в большой двоичный объект. Порядок действий следующий.

1. Процесс определит, что новые данные были отправлены.  В приведенном примере используется [приложение логики Azure](../logic-apps/logic-apps-overview.md), обладающее триггером обнаружения новых данных, которые были отправлены в большой двоичный объект.

2. Процессор считывает эти данные и преобразует их в формат JSON, необходимый для службы Log Analytics.  [Функции Azure](../azure-functions/functions-overview.md), используемые в этом примере, являются легким и экономичным способом выполнения обработки сведений. Эта функция запускается одним и тем же логическим приложением, которое было использовано для обнаружения новых данных.

3. Как только объект JSON станет доступен, он отправляется в службу Log Analytics. Одно и то же приложение логики использует встроенное действие сборщика данных службы Log Analytics для отправки данных в Log Analytics.

Хотя подробные установки хранилища BLOB-объектов, приложения логики или Функции Azure не описаны в этой статье, подробные инструкции к ним доступны на страницах указанных продуктов.

Для отслеживания конвейера используется средство Application Insights для отслеживания Функции Azure [(дополнительные сведения см. здесь)](../azure-functions/functions-monitoring.md) и служба Log Analytics для отслеживания приложения логики [(дополнительные сведения см. здесь)](../logic-apps/logic-apps-monitor-your-logic-apps-oms.md). 

## <a name="setting-up-the-pipeline"></a>Установка конвейера
Чтобы задать конвейер, сначала необходимо убедиться, что контейнер больших двоичных объектов создан и настроен. Аналогичным образом следует убедиться, что была создана рабочая область Log Analytics, в которою планируется отправлять данные.

## <a name="ingesting-json-data"></a>Прием данных JSON
Прием данных в формате JSON с помощью приложений логики является обычным процессом, а поскольку преобразование здесь не требуется, то весь конвейер можно добавить в одно приложение логики. После настройки контейнера больших двоичных объектов и рабочей области Log Analytics создайте новое приложение логики и настройте его следующим образом.

![Примеры рабочего процесса приложений логики](./media/log-analytics-create-pipeline-datacollector-api/logic-apps-workflow-example-01.png)

Сохраните приложение логики и перейдите к его проверке.

## <a name="ingesting-xml-csv-or-other-formats-of-data"></a>Прием данных форматов XML, CSV или других форматов
На данный момент в Logic Apps отсутствуют встроенные возможности преобразования форматов XML, CSV или других типов форматов в JSON. Таким образом, для выполнения этого преобразования требуется использование сторонних средств. В этой статье были использованы возможности бессерверных вычислений Функций Azure, которые является очень легким и дешевым способом это реализовать. 

В этом примере был проведен анализ CSV-файла. При этом любой другой тип файла можно обрабатывать аналогичным образом. Для этого следует изменить десериализующую часть функции Azure, что позволит отобразить правильную логику указанного типа данных.

1.  При появлении запроса создайте новую функцию Azure, используя Функцию среды выполнения версии 1 и потребление.  В качестве отправной точки C#, которая настраивает привязки по мере необходимости, выберите шаблон **Триггер HTTP**. 
2.  Чтобы создать файл **project.json** и вставить следующий код из используемого пакета NuGet, используйте вкладку **Просмотреть файлы**, находящуюся в правой области панели.

    ![Пример проекта Функций Azure](./media/log-analytics-create-pipeline-datacollector-api/functions-example-project-01.png)
    
    ``` JSON
    {
      "frameworks": {
        "net46":{
          "dependencies": {
            "CsvHelper": "7.1.1",
            "Newtonsoft.Json": "11.0.2"
          }  
        }  
       }  
     }  
    ```

3. В правой области панели перейдите к файлу **run.csx** и замените код, который находится там по умолчанию, следующим кодом. 

    >[!NOTE]
    >Для текущего проекта требуется заменить модель записи (класс PopulationRecord) собственной схемой данных.
    >

    ```   
    using System.Net;
    using Newtonsoft.Json;
    using CsvHelper;
    
    class PopulationRecord
    {
        public String Location { get; set; }
        public int Time { get; set; }
        public long Population { get; set; }
    }

    public static async Task<HttpResponseMessage> Run(HttpRequestMessage req, TraceWriter log)
    {
        string filePath = await req.Content.ReadAsStringAsync(); //get the CSV URI being passed from Logic App
        string response = "";

        //get a stream from blob
        WebClient wc = new WebClient();
        Stream s = wc.OpenRead(filePath);         

        //read the stream
        using (var sr = new StreamReader(s))
        {
            var csvReader = new CsvReader(sr);
    
            var records = csvReader.GetRecords<PopulationRecord>(); //deserialize the CSV stream as an IEnumerable
    
            response = JsonConvert.SerializeObject(records); //serialize the IEnumerable back into JSON
        }    

        return response == null
            ? req.CreateResponse(HttpStatusCode.BadRequest, "There was an issue getting data")
            : req.CreateResponse(HttpStatusCode.OK, response);
     }  
    ```

4. Сохраните функцию.
5. Чтобы убедиться, что код работает правильно, проверьте функцию. Перейдите на вкладку **Тест** в правой области панели, настроив тестирование следующим образом. Ссылку следует поместить в текстовое поле **Тело запроса** большого двоичного объекта, в котором находится пример данных. Щелкнув **Выполнить**, в поле **Вывод** должны появиться выходные данные JSON.

    ![Проверка кода в приложении-функции](./media/log-analytics-create-pipeline-datacollector-api/functions-test-01.png)

Теперь необходимо вернуться назад и изменить приложение логики, которое создавалось ранее, чтобы включить данные, полученные и преобразованные в формат JSON.  Используйте "Конструктор представлений", чтобы настроить и сохранить приложение логики следующим образом.

![Полный пример рабочего процесса приложения логики](./media/log-analytics-create-pipeline-datacollector-api/logic-apps-workflow-example-02.png)

## <a name="testing-the-pipeline"></a>Тестирование конвейера
Теперь новый файл может быть отправлен в ранее установленный большой двоичный объект, а его мониторинг можно выполнять с помощью приложения логики. Вскоре появится новый экземпляр запущенного приложения логики, который вызывает функцию Azure, а затем успешно отправляет данные в Log Analytics. 

>[!NOTE]
>При первой отправке нового типа данных их отображение в Log Analytics может занять до 30 минут.


## <a name="correlating-with-other-data-in-log-analytics-and-application-insights"></a>Сопоставление с другими данными в Log Analytics и Application Insights
Чтобы завершить задачу по сопоставлению данных просмотра страницы Application Insights с данными о населении, которые были приняты от пользовательского источника данных, необходимо выполнить следующий запрос из окна аналитики Application Insights или из рабочего пространства Log Analytics.

``` KQL
app("fabrikamprod").pageViews
| summarize numUsers = count() by client_CountryOrRegion
| join kind=leftouter (
   workspace("customdatademo").Population_CL
) on $left.client_CountryOrRegion == $right.Location_s
| project client_CountryOrRegion, numUsers, Population_d
```

В выходных данных должны отображаться два присоединенных источника данных.  

![Пример сопоставления отсоединенных данных в результатах поиска](./media/log-analytics-create-pipeline-datacollector-api/correlating-disjoined-data-example-01.png)

## <a name="suggested-improvements-for-a-production-pipeline"></a>Предлагаемые улучшения для рабочего конвейера
В этой статье представлен рабочий прототип, логика которого может быть применена в истинном решении промышленного качества. Для такого решения промышленного качества рекомендуется использовать следующие улучшения.

* Добавьте обработку ошибок и логику повторных попыток в приложение логики и функцию Azure.
* Добавьте логику, чтобы гарантировать, что не превышено ограничение на 30 МБ за один вызов API приема данных от Log Analytics. При необходимости разделите данные на сегменты меньшего размера.
* Настройте политику очистки облачного хранилища. Если нет необходимости хранить необработанные данные с архивными целями, то после успешной отправки данных в Log Analytics нет никаких оснований продолжать их хранение. 
* Включите мониторинг по всему конвейеру. При необходимости можно добавить точки трассировки и предупреждения.
* Используйте системы управления версиями для управления кодом функции и приложения логики.
* Убедитесь, что соблюдена правильная политика управления изменениями, которая позволяет изменять схемы, функции и приложения логики соответствующим образом.
* Отправляя несколько разных типов данных, разделите их на отдельные папки в контейнере больших двоичных объектов и создайте логику, чтобы разделить логику на основе типа данных. 


## <a name="next-steps"></a>Дополнительная информация
Дополнительные сведения о том, как записать данные из любого клиента REST API в Log Analytics, см. в статье [Отправка данных в Log Analytics с помощью API сборщика данных HTTP (общедоступная предварительная версия)](log-analytics-data-collector-api.md).
